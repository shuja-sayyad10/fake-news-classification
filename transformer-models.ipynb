{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### team members: \n",
    "- Syed Razauddin Shahlal ;         ubid: 50496396\n",
    "- Tajammul Shuja Sayyad  ;         ubid: 50495179  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers based model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-06T01:31:57.618847Z",
     "iopub.status.busy": "2023-12-06T01:31:57.618478Z",
     "iopub.status.idle": "2023-12-06T01:31:57.975013Z",
     "shell.execute_reply": "2023-12-06T01:31:57.973947Z",
     "shell.execute_reply.started": "2023-12-06T01:31:57.618814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fakenews/WELFake_Dataset.csv\n",
      "/kaggle/input/fake-news-classification/WELFake_Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### install the required libraries. We will be using hugging face transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:00.098312Z",
     "iopub.status.busy": "2023-12-06T01:32:00.097793Z",
     "iopub.status.idle": "2023-12-06T01:32:13.041969Z",
     "shell.execute_reply": "2023-12-06T01:32:13.041040Z",
     "shell.execute_reply.started": "2023-12-06T01:32:00.098277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.0)\n",
      "Collecting evaluate\n",
      "  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.10.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:16.906655Z",
     "iopub.status.busy": "2023-12-06T01:32:16.905830Z",
     "iopub.status.idle": "2023-12-06T01:32:16.910996Z",
     "shell.execute_reply": "2023-12-06T01:32:16.910045Z",
     "shell.execute_reply.started": "2023-12-06T01:32:16.906614Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:18.472381Z",
     "iopub.status.busy": "2023-12-06T01:32:18.471633Z",
     "iopub.status.idle": "2023-12-06T01:32:24.076476Z",
     "shell.execute_reply": "2023-12-06T01:32:24.075497Z",
     "shell.execute_reply.started": "2023-12-06T01:32:18.472347Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"/kaggle/input/fakenews/WELFake_Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:25.105388Z",
     "iopub.status.busy": "2023-12-06T01:32:25.105016Z",
     "iopub.status.idle": "2023-12-06T01:32:25.150808Z",
     "shell.execute_reply": "2023-12-06T01:32:25.149948Z",
     "shell.execute_reply.started": "2023-12-06T01:32:25.105348Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop nan values from both text and title as there are very few missing values so we will remove those rows\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check the distribution of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:27.065331Z",
     "iopub.status.busy": "2023-12-06T01:32:27.064953Z",
     "iopub.status.idle": "2023-12-06T01:32:27.083941Z",
     "shell.execute_reply": "2023-12-06T01:32:27.082787Z",
     "shell.execute_reply.started": "2023-12-06T01:32:27.065300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    36509\n",
       "0    35028\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check label distribution--> equal distribution for both classes.\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting our data(70-15-15)\n",
    "- training data: used for fine tuning the models\n",
    "- validation data: for hyperparameters tuning\n",
    "- testing data: for checking accuracy and measuring generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:29.013398Z",
     "iopub.status.busy": "2023-12-06T01:32:29.013038Z",
     "iopub.status.idle": "2023-12-06T01:32:29.514260Z",
     "shell.execute_reply": "2023-12-06T01:32:29.513260Z",
     "shell.execute_reply.started": "2023-12-06T01:32:29.013369Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#we will divide our dataset into train validation and test sets. \n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.3,random_state=42)\n",
    "test_df, eval_df = train_test_split(test_df, test_size=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for hugging face transformers we will need Dataset object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:31.723654Z",
     "iopub.status.busy": "2023-12-06T01:32:31.723232Z",
     "iopub.status.idle": "2023-12-06T01:32:32.646564Z",
     "shell.execute_reply": "2023-12-06T01:32:32.645787Z",
     "shell.execute_reply.started": "2023-12-06T01:32:31.723618Z"
    }
   },
   "outputs": [],
   "source": [
    "# for hugging face transformers we will need Dataset object\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:34.440125Z",
     "iopub.status.busy": "2023-12-06T01:32:34.439424Z",
     "iopub.status.idle": "2023-12-06T01:32:35.185760Z",
     "shell.execute_reply": "2023-12-06T01:32:35.184810Z",
     "shell.execute_reply.started": "2023-12-06T01:32:34.440081Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "#convert our data from dataframe to datasets(for hugging face library implementations)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:37.289264Z",
     "iopub.status.busy": "2023-12-06T01:32:37.288863Z",
     "iopub.status.idle": "2023-12-06T01:32:37.296210Z",
     "shell.execute_reply": "2023-12-06T01:32:37.295120Z",
     "shell.execute_reply.started": "2023-12-06T01:32:37.289234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'title', 'text', 'label', '__index_level_0__'],\n",
       "    num_rows: 50075\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:40.246270Z",
     "iopub.status.busy": "2023-12-06T01:32:40.245899Z",
     "iopub.status.idle": "2023-12-06T01:32:40.256793Z",
     "shell.execute_reply": "2023-12-06T01:32:40.255720Z",
     "shell.execute_reply.started": "2023-12-06T01:32:40.246240Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will remove unwanted columns which we dont require for prediction.\n",
    "train_dataset=train_dataset.remove_columns(['Unnamed: 0','__index_level_0__'])\n",
    "test_dataset=test_dataset.remove_columns(['Unnamed: 0','__index_level_0__'])\n",
    "eval_dataset=eval_dataset.remove_columns(['Unnamed: 0','__index_level_0__'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will first try just with the title column and experiment with different models to get a sense how each model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:42.242125Z",
     "iopub.status.busy": "2023-12-06T01:32:42.241756Z",
     "iopub.status.idle": "2023-12-06T01:32:42.246497Z",
     "shell.execute_reply": "2023-12-06T01:32:42.245519Z",
     "shell.execute_reply.started": "2023-12-06T01:32:42.242095Z"
    }
   },
   "outputs": [],
   "source": [
    "#first try with just title to predict(epoch=2)-->distilebert,bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:44.276125Z",
     "iopub.status.busy": "2023-12-06T01:32:44.275322Z",
     "iopub.status.idle": "2023-12-06T01:32:49.749132Z",
     "shell.execute_reply": "2023-12-06T01:32:49.748347Z",
     "shell.execute_reply.started": "2023-12-06T01:32:44.276085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698d3e355b7c44959ea47c5b7c89b053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa437091c1bd409684d488bb8eca9b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2794935049c64e6a8c6c2130128dc9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278ded6f39e54449b49e00445a8df010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# each pretrained transformer model comes with its pretrained tokenizer.\n",
    "# we will use these tokenizers to tokenize our data so that it aligns with model's input requirements.\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### define a preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:52.378951Z",
     "iopub.status.busy": "2023-12-06T01:32:52.378425Z",
     "iopub.status.idle": "2023-12-06T01:32:52.383655Z",
     "shell.execute_reply": "2023-12-06T01:32:52.382506Z",
     "shell.execute_reply.started": "2023-12-06T01:32:52.378920Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a preprocess function that takes the dataset as parameter and then we will tokenize 'title' col.\n",
    "# truncation is set to true so that if a text has token length more than models token length limit it will be truncated(512 for bert)\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"title\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tokenize each of the datasets.\n",
    "- To process dataset in one step, use hugging face Datasets map method to apply a preprocessing function over the entire dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:32:55.393698Z",
     "iopub.status.busy": "2023-12-06T01:32:55.393341Z",
     "iopub.status.idle": "2023-12-06T01:33:00.196924Z",
     "shell.execute_reply": "2023-12-06T01:33:00.195850Z",
     "shell.execute_reply.started": "2023-12-06T01:32:55.393668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c95a13931ff4108abe81f87cbc37f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30f2f4fee744ab69f5585c166e93b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864ba7bbdc7f49d4ad5340e4cd18f8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:33:03.398115Z",
     "iopub.status.busy": "2023-12-06T01:33:03.397415Z",
     "iopub.status.idle": "2023-12-06T01:33:03.404157Z",
     "shell.execute_reply": "2023-12-06T01:33:03.403045Z",
     "shell.execute_reply.started": "2023-12-06T01:33:03.398082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 50075\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see that distil-bert tokenized the data and added input_ids and attention_mask as features.\n",
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  looking at input_ids features:\n",
    "- 101 :[cls] token(added at beginning of each sentence) and \n",
    "- 102 :[sep] token(Added at the end) added by distel-bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:33:05.831428Z",
     "iopub.status.busy": "2023-12-06T01:33:05.831064Z",
     "iopub.status.idle": "2023-12-06T01:33:05.838852Z",
     "shell.execute_reply": "2023-12-06T01:33:05.837856Z",
     "shell.execute_reply.started": "2023-12-06T01:33:05.831397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 8499,\n",
       " 787,\n",
       " 188,\n",
       " 5055,\n",
       " 1113,\n",
       " 15978,\n",
       " 13429,\n",
       " 24442,\n",
       " 7426,\n",
       " 4785,\n",
       " 6424,\n",
       " 6335,\n",
       " 2409,\n",
       " 7661,\n",
       " 787,\n",
       " 188,\n",
       " 118,\n",
       " 1109,\n",
       " 1203,\n",
       " 1365,\n",
       " 2706,\n",
       " 102]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can look at an example. 101 is the cls token and 102 sep token added by distel-bert.\n",
    "tokenized_train_dataset[0]['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data collator for padding:\n",
    "- It’s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:33:08.496579Z",
     "iopub.status.busy": "2023-12-06T01:33:08.495740Z",
     "iopub.status.idle": "2023-12-06T01:33:17.942829Z",
     "shell.execute_reply": "2023-12-06T01:33:17.941994Z",
     "shell.execute_reply.started": "2023-12-06T01:33:08.496545Z"
    }
   },
   "outputs": [],
   "source": [
    "# data collator is required for data of varied lengths.\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:33:19.629632Z",
     "iopub.status.busy": "2023-12-06T01:33:19.628396Z",
     "iopub.status.idle": "2023-12-06T01:33:22.015399Z",
     "shell.execute_reply": "2023-12-06T01:33:22.014383Z",
     "shell.execute_reply.started": "2023-12-06T01:33:19.629593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c955a7cb373481682d1a3aea1caf483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we will use accuracy as the evaluation metric\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### define computing metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:33:25.788291Z",
     "iopub.status.busy": "2023-12-06T01:33:25.787908Z",
     "iopub.status.idle": "2023-12-06T01:33:25.793640Z",
     "shell.execute_reply": "2023-12-06T01:33:25.792726Z",
     "shell.execute_reply.started": "2023-12-06T01:33:25.788263Z"
    }
   },
   "outputs": [],
   "source": [
    "# models output are raw logits so we will have to convert them into predicted labels by using argmax function.\n",
    "# essentially we have an output that we get after model applies softmax function to the last layer/output layer(Also called head)\n",
    "# these logits values are probabilty distribution.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:33:27.851053Z",
     "iopub.status.busy": "2023-12-06T01:33:27.850655Z",
     "iopub.status.idle": "2023-12-06T01:33:27.855748Z",
     "shell.execute_reply": "2023-12-06T01:33:27.854925Z",
     "shell.execute_reply.started": "2023-12-06T01:33:27.851021Z"
    }
   },
   "outputs": [],
   "source": [
    "#create a dictionary to represent class labels.\n",
    "id2label = {0: \"FAKE\", 1: \"REAL\"}\n",
    "label2id = {\"FAKE\": 0, \"REAL\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will first try Distilbert model which is a lighter version of bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:33:29.989135Z",
     "iopub.status.busy": "2023-12-06T01:33:29.988410Z",
     "iopub.status.idle": "2023-12-06T01:33:33.044548Z",
     "shell.execute_reply": "2023-12-06T01:33:33.043763Z",
     "shell.execute_reply.started": "2023-12-06T01:33:29.989099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc38c4d22284742ab1cc3ad87f8cf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136ce4384a9642ae8b18eb0cc6d6006c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load the distilbert model\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:33:35.744764Z",
     "iopub.status.busy": "2023-12-06T01:33:35.744381Z",
     "iopub.status.idle": "2023-12-06T01:33:35.750713Z",
     "shell.execute_reply": "2023-12-06T01:33:35.749677Z",
     "shell.execute_reply.started": "2023-12-06T01:33:35.744730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"FAKE\",\n",
      "    \"1\": \"REAL\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"FAKE\": 0,\n",
      "    \"REAL\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.35.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can check the models architecture\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:33:40.190836Z",
     "iopub.status.busy": "2023-12-06T01:33:40.189993Z",
     "iopub.status.idle": "2023-12-06T01:33:40.200551Z",
     "shell.execute_reply": "2023-12-06T01:33:40.199463Z",
     "shell.execute_reply.started": "2023-12-06T01:33:40.190806Z"
    }
   },
   "outputs": [],
   "source": [
    "# as first we are trying just by title column we can drop text column. Although the Trainer class can take of this itself.\n",
    "tokenized_train_dataset=tokenized_train_dataset.remove_columns(['text'])\n",
    "tokenized_eval_dataset=tokenized_eval_dataset.remove_columns(['text'])\n",
    "tokenized_test_dataset=tokenized_test_dataset.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate Training arguments and Trainer\n",
    "- Training Arguments: create a TrainingArguments class which contains all the hyperparameters you can tune as well as flags for activating different training options.\n",
    "\n",
    "- Trainer class: Hugging face Transformers provides a Trainer class optimized for training Transformers models, making it easier to start training without manually writing your own training loop. The Trainer API supports a wide range of training options and features such as logging, gradient accumulation, and mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:33:43.617524Z",
     "iopub.status.busy": "2023-12-06T01:33:43.616807Z",
     "iopub.status.idle": "2023-12-06T01:33:43.621906Z",
     "shell.execute_reply": "2023-12-06T01:33:43.620819Z",
     "shell.execute_reply.started": "2023-12-06T01:33:43.617490Z"
    }
   },
   "outputs": [],
   "source": [
    "# we start with 2 epochs to build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:33:47.199041Z",
     "iopub.status.busy": "2023-12-06T01:33:47.198642Z",
     "iopub.status.idle": "2023-12-06T01:42:59.749744Z",
     "shell.execute_reply": "2023-12-06T01:42:59.748863Z",
     "shell.execute_reply.started": "2023-12-06T01:33:47.199003Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20231206_013412-gc6e9egi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gunners/huggingface/runs/gc6e9egi' target=\"_blank\">vital-puddle-6</a></strong> to <a href='https://wandb.ai/gunners/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gunners/huggingface' target=\"_blank\">https://wandb.ai/gunners/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gunners/huggingface/runs/gc6e9egi' target=\"_blank\">https://wandb.ai/gunners/huggingface/runs/gc6e9egi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3130' max='3130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3130/3130 08:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.155389</td>\n",
       "      <td>0.937937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.145969</td>\n",
       "      <td>0.944274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3130, training_loss=0.16893480501997585, metrics={'train_runtime': 546.6381, 'train_samples_per_second': 183.211, 'train_steps_per_second': 5.726, 'total_flos': 1377687077286588.0, 'train_loss': 0.16893480501997585, 'epoch': 2.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_1\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:43:17.529583Z",
     "iopub.status.busy": "2023-12-06T01:43:17.529209Z",
     "iopub.status.idle": "2023-12-06T01:43:33.945737Z",
     "shell.execute_reply": "2023-12-06T01:43:33.944780Z",
     "shell.execute_reply.started": "2023-12-06T01:43:17.529552Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1364707350730896,\n",
       " 'eval_accuracy': 0.9485602460162147,\n",
       " 'eval_runtime': 16.4035,\n",
       " 'eval_samples_per_second': 654.191,\n",
       " 'eval_steps_per_second': 20.483,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next we check accuracy on test data by looking at eval_accuracy.\n",
    "trainer.evaluate(tokenized_test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "- just by using title column and fine tuning distilbert we are getting accuracy of around 94% for validation and around 95% for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we try bert with title column\n",
    "- for bert we use its own tokenizer which it was pre trained on to get the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:43:49.133738Z",
     "iopub.status.busy": "2023-12-06T01:43:49.132828Z",
     "iopub.status.idle": "2023-12-06T01:43:58.438913Z",
     "shell.execute_reply": "2023-12-06T01:43:58.437815Z",
     "shell.execute_reply.started": "2023-12-06T01:43:49.133704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f959e17020340ed824d90a4d8c427b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e0182298114cc79abfce2636fdf139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80cb52b594f47b39c424e72f06707e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52556fbe808743a993a4dffac59d04ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab65115a2be4c08b5c81bbc98f4492b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e37c6bf14db48d997f7a607c7737b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79d9113ede645d7ac89328dd7f67109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c947a5551cca4ddfbce6ccef00c80ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2baccbf5e742958583bdc7005af62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#with bert with just title column\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "tokenized_train_dataset=tokenized_train_dataset.remove_columns(['text'])\n",
    "tokenized_eval_dataset=tokenized_eval_dataset.remove_columns(['text'])\n",
    "tokenized_test_dataset=tokenized_test_dataset.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:44:04.378507Z",
     "iopub.status.busy": "2023-12-06T01:44:04.378142Z",
     "iopub.status.idle": "2023-12-06T01:59:34.362335Z",
     "shell.execute_reply": "2023-12-06T01:59:34.360039Z",
     "shell.execute_reply.started": "2023-12-06T01:44:04.378477Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3130' max='3130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3130/3130 15:29, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.158426</td>\n",
       "      <td>0.938589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.150940</td>\n",
       "      <td>0.946324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3130, training_loss=0.16876022960431278, metrics={'train_runtime': 929.4071, 'train_samples_per_second': 107.757, 'train_steps_per_second': 3.368, 'total_flos': 2736406878510780.0, 'train_loss': 0.16876022960431278, 'epoch': 2.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_1\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T01:59:49.969674Z",
     "iopub.status.busy": "2023-12-06T01:59:49.969301Z",
     "iopub.status.idle": "2023-12-06T02:00:18.855170Z",
     "shell.execute_reply": "2023-12-06T02:00:18.854032Z",
     "shell.execute_reply.started": "2023-12-06T01:59:49.969642Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.14027650654315948,\n",
       " 'eval_accuracy': 0.950796757058988,\n",
       " 'eval_runtime': 28.8723,\n",
       " 'eval_samples_per_second': 371.671,\n",
       " 'eval_steps_per_second': 11.637,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### observations:\n",
    "- we observe slight better accuracy for both test and validation with bert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using both text and title columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:00:23.828076Z",
     "iopub.status.busy": "2023-12-06T02:00:23.827328Z",
     "iopub.status.idle": "2023-12-06T02:00:23.833543Z",
     "shell.execute_reply": "2023-12-06T02:00:23.832101Z",
     "shell.execute_reply.started": "2023-12-06T02:00:23.828042Z"
    }
   },
   "outputs": [],
   "source": [
    "# we get slight improvement just by using the title column. Next we will combine both title and text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### distilbert with both title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:00:27.046741Z",
     "iopub.status.busy": "2023-12-06T02:00:27.046011Z",
     "iopub.status.idle": "2023-12-06T02:00:27.052519Z",
     "shell.execute_reply": "2023-12-06T02:00:27.051438Z",
     "shell.execute_reply.started": "2023-12-06T02:00:27.046709Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"title\"],examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:00:29.155408Z",
     "iopub.status.busy": "2023-12-06T02:00:29.154690Z",
     "iopub.status.idle": "2023-12-06T02:00:29.283412Z",
     "shell.execute_reply": "2023-12-06T02:00:29.282488Z",
     "shell.execute_reply.started": "2023-12-06T02:00:29.155377Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:00:33.036384Z",
     "iopub.status.busy": "2023-12-06T02:00:33.035510Z",
     "iopub.status.idle": "2023-12-06T02:01:44.802280Z",
     "shell.execute_reply": "2023-12-06T02:01:44.801305Z",
     "shell.execute_reply.started": "2023-12-06T02:00:33.036341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cc0266e8674ace84db995a2da42b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c84ba6befb45fa9b0879ac13d22598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a78fd810148407193c413f868f9995f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When we comine text and title berts tokenizer adds speacial token like cls at the beginning and sep token at the end of one text data.\n",
    "- To differentiate between title and text bert added [sep] token after title ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:02:44.866373Z",
     "iopub.status.busy": "2023-12-06T02:02:44.866006Z",
     "iopub.status.idle": "2023-12-06T02:02:44.884707Z",
     "shell.execute_reply": "2023-12-06T02:02:44.883810Z",
     "shell.execute_reply.started": "2023-12-06T02:02:44.866343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Trump ’ s Take on Corporate Tax Rate Could Look Very Much Like Obama ’ s - The New York Times [SEP] Just a few days after being sworn in as president, Donald J. Trump convened a meeting at the White House of some of the nation ’ s most prominent chief executives to discuss how to improve manufacturing. Mr. Trump was joined by Elon Musk of Tesla, Mark Fields of Ford, Andrew Liveris of Dow Chemical, Marillyn A. Hewson of Lockheed Martin and Michael Dell of Dell, among others. Before the meeting formally began, with cameras rolling, Mr. Trump wanted to talk about corporate tax rates — specifically lowering them. During one debate with Hillary Clinton, he had told voters : “ Under my plan, I ’ ll be reducing taxes tremendously, from 35 percent to 15 percent for companies, small and big businesses. That ’ s going to be a job creator like we haven ’ t seen since Ronald Reagan. It ’ s going to be a beautiful thing to watch. ” He repeated the 15 percent figure over and over again. However, when Mr. Trump spoke to the C. E. O. s that morning, he shifted the goal post slightly : “ We ’ re trying to get it down to anywhere from 15 to 20 percent. ” Inside the White House, until last Friday, according to people involved in the conversations, the target rate had been bumped up again, to a minimum of 20 percent and very likely a bit more. And now, given that the repeal of the Affordable Care Act is off the table — and with it the $ 1 trillion in tax cuts over the next 10 years that the administration needed to help make its tax plan deficit neutral — there is a good chance that any tax package would include a corporate rate that is even further from Mr. Trump ’ s initial pledge, perhaps as high as 28 percent. Don ’ t take my word for it. That ’ s the figure that Grover Norquist, the activist who is the president of Americans for Tax Reform, and who has long sought the lowest rate possible, has calculated would be needed for the plan to be deficit neutral after 10 years. That way it could come under the heading of budget reconciliation, which would allow the Senate to pass legislation with a simple majority. If the 28 percent corporate tax rate sounds familiar, that ’ s because it is : It was the same rate that President Barack Obama proposed in 2012 and again in 2013, when he also proposed to lower the tax rate for United States manufacturers [SEP]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets have a look how does the features look like after tokenizing both text and title.\n",
    "#bert's tokenizer adds special tokens to differentiate different texts. \n",
    "#Here we can see title and text are seperated by SEP token.\n",
    "tokenizer.decode(tokenized_train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:03:56.715759Z",
     "iopub.status.busy": "2023-12-06T02:03:56.714861Z",
     "iopub.status.idle": "2023-12-06T02:03:57.415736Z",
     "shell.execute_reply": "2023-12-06T02:03:57.414617Z",
     "shell.execute_reply.started": "2023-12-06T02:03:56.715725Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:04:08.071460Z",
     "iopub.status.busy": "2023-12-06T02:04:08.071023Z",
     "iopub.status.idle": "2023-12-06T02:28:52.178421Z",
     "shell.execute_reply": "2023-12-06T02:28:52.177376Z",
     "shell.execute_reply.started": "2023-12-06T02:04:08.071425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1565' max='1565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1565/1565 24:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.072400</td>\n",
       "      <td>0.065378</td>\n",
       "      <td>0.979965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1565, training_loss=0.1604620979235957, metrics={'train_runtime': 1483.3769, 'train_samples_per_second': 33.757, 'train_steps_per_second': 1.055, 'total_flos': 6633304987699200.0, 'train_loss': 0.1604620979235957, 'epoch': 1.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:29:26.221609Z",
     "iopub.status.busy": "2023-12-06T02:29:26.220886Z",
     "iopub.status.idle": "2023-12-06T02:31:17.345660Z",
     "shell.execute_reply": "2023-12-06T02:31:17.344322Z",
     "shell.execute_reply.started": "2023-12-06T02:29:26.221573Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 01:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0713673010468483,\n",
       " 'eval_accuracy': 0.9778212654924984,\n",
       " 'eval_runtime': 111.1106,\n",
       " 'eval_samples_per_second': 96.579,\n",
       " 'eval_steps_per_second': 3.024,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on test data\n",
    "trainer.evaluate(tokenized_test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### observations:\n",
    "- We get 97% accuracy after 1 epoch when we combine text and title columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### finally we use bert on both the features(title+text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:31:34.477454Z",
     "iopub.status.busy": "2023-12-06T02:31:34.476700Z",
     "iopub.status.idle": "2023-12-06T02:32:51.435322Z",
     "shell.execute_reply": "2023-12-06T02:32:51.434302Z",
     "shell.execute_reply.started": "2023-12-06T02:31:34.477421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe710dc3f8c4547a3b98aa0cc52aaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023c56ff8c0a492e89d87542c5b84b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f89033343034d14a434fe61d69d6ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:32:55.714979Z",
     "iopub.status.busy": "2023-12-06T02:32:55.714553Z",
     "iopub.status.idle": "2023-12-06T02:32:55.723749Z",
     "shell.execute_reply": "2023-12-06T02:32:55.722617Z",
     "shell.execute_reply.started": "2023-12-06T02:32:55.714944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 50075\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with bert tokenizer we get an additional feature known as token_type_ids\n",
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### understanding bert's tokenizers features:\n",
    "- input_ids: these are numbers that are given to each word in our data according to the bert's vocabulary.\n",
    "- attention masks: tells bert that if a text was padded(to have a fixed length input) it should not be focused on. It uses binary list of 1's and 0's. 0 tells us that the word was padded.\n",
    "- token_type_ids: model can differentiate between title input_ids and text input_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:33:55.156426Z",
     "iopub.status.busy": "2023-12-06T02:33:55.156060Z",
     "iopub.status.idle": "2023-12-06T02:33:55.176664Z",
     "shell.execute_reply": "2023-12-06T02:33:55.175608Z",
     "shell.execute_reply.started": "2023-12-06T02:33:55.156397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is a list with 0 and 1 which tells model which input_ids belong to which column. \n",
    "#0 here is for title and 1 for text column input_ids.\n",
    "#this way model can know if there are multiple text inputs and can differentiate them.\n",
    "tokenized_train_dataset[0]['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:37:53.690024Z",
     "iopub.status.busy": "2023-12-06T02:37:53.689244Z",
     "iopub.status.idle": "2023-12-06T02:37:54.870785Z",
     "shell.execute_reply": "2023-12-06T02:37:54.869629Z",
     "shell.execute_reply.started": "2023-12-06T02:37:53.689990Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T02:38:21.154884Z",
     "iopub.status.busy": "2023-12-06T02:38:21.154519Z",
     "iopub.status.idle": "2023-12-06T03:27:30.139277Z",
     "shell.execute_reply": "2023-12-06T03:27:30.137097Z",
     "shell.execute_reply.started": "2023-12-06T02:38:21.154841Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1565' max='1565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1565/1565 49:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.063986</td>\n",
       "      <td>0.982108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1565, training_loss=0.1292401312258297, metrics={'train_runtime': 2948.1508, 'train_samples_per_second': 16.985, 'train_steps_per_second': 0.531, 'total_flos': 1.3175286097152e+16, 'train_loss': 0.1292401312258297, 'epoch': 1.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_3\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T03:28:06.336946Z",
     "iopub.status.busy": "2023-12-06T03:28:06.336566Z",
     "iopub.status.idle": "2023-12-06T03:31:44.650965Z",
     "shell.execute_reply": "2023-12-06T03:31:44.649746Z",
     "shell.execute_reply.started": "2023-12-06T03:28:06.336915Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 03:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.06537528336048126,\n",
       " 'eval_accuracy': 0.9819215357375827,\n",
       " 'eval_runtime': 218.3003,\n",
       " 'eval_samples_per_second': 49.157,\n",
       " 'eval_steps_per_second': 1.539,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next we check accuracy on test data by looking at eval_accuracy.\n",
    "\n",
    "trainer.evaluate(tokenized_test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "- Just by training for 1 epoch we can see bert is doing a great job and we are getting 98% accuracy for both validation and test data\n",
    "- Bert with both the features is doing the best job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FInal thoughts:\n",
    "- Depending on the resources and time available we can train the model for even more epochs and apply early stopping if we see that the model's performance is not changing over time.\n",
    "- We can also look at models that are more sophisticated than bert like roberta, XL-Net and transformer-XL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### references:\n",
    "- https://huggingface.co/docs/transformers/index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2093157,
     "sourceId": 6641062,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4095189,
     "sourceId": 7103778,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
